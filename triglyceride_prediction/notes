
        random_search = RandomizedSearchCV(model, param_distributions=param_distributions[name], n_iter=25,
                                           scoring='neg_mean_absolute_error', cv=4, n_jobs=-1, random_state=42)

   #tune these values


   ===========

   Key Changes and Improvements:

Missing Value Imputation:

You correctly applied the SimpleImputer with strategy='mean' to handle missing values in both the training set (X_train) and the test set (X_test) after the encoding step.
Splitting and Transformation Order:

You first split the train_data into X_train and y_train before applying the ColumnTransformer.
This is the correct order as you want to avoid data leakage. You should fit the transformer only on the training data and then use it to transform both the training and test data.
Validation Set Imputation:

You applied the imputer.transform method to X_val to handle any potential missing values that might arise in your validation set after splitting the data.
With these changes, your code should run without errors and produce a valid Linear Regression model. You can now proceed to evaluate the model's performance, potentially try other models, and finally make predictions on the test set.

Additional Tips:

Experiment with Different Models:
Consider trying other regression algorithms (e.g., Ridge, Lasso, Random Forest, Gradient Boosting) to see if you can achieve better performance than Linear Regression.
Hyperparameter Tuning:
If you have time, fine-tune the hyperparameters of your model(s) using techniques like Grid Search or Random Search to optimize the performance further.
Error Analysis:
After evaluating your models, analyze the types of errors they are making. This can give you valuable insights on how to improve your features or choose a more appropriate model.

===========

import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error

# Load data (since files are in the same directory)
train_data = pd.read_csv("train.csv")
test_data = pd.read_csv("test.csv")

# Basic information
# print("Train data shape:", train_data.shape)
# print("Test data shape:", test_data.shape)
# print(train_data.head().to_markdown(index=False, numalign="left", stralign="left"))

# Check data types
# print("\nData types:")
# print(train_data.dtypes)

# Check for missing values
# print("\nMissing values in train data:")
# print(train_data.isnull().sum())

# print("\nMissing values in test data:")
# print(test_data.isnull().sum())

# Summary statistics
# print("\nSummary statistics of train data:")
# print(train_data.describe().to_markdown())

# Check distribution of target variable
# print("\nDistribution of triglyceride_lvl:")
# print(train_data['triglyceride_lvl'].describe())

# Check unique values for categorical columns
for col in train_data.select_dtypes(include='object'):
     print(f'\nUnique values for {col}:')
    # print(train_data[col].unique())

# 1. Outlier Handling:
# Define columns to check for outliers
outlier_cols = ['triglyceride_lvl', 'total_cholestrol', 'good_cholestrol_lvl', 'bad_cholestrol_lvl', 'liver_enzyme_lvl1']

# Function to cap outliers at the 99th percentile
def cap_outliers(df, columns):
    for col in columns:
        percentile_99 = df[col].quantile(0.99)
        df[col] = df[col].clip(upper=percentile_99)
    return df

# Cap outliers in the training data
train_data = cap_outliers(train_data, outlier_cols)


# 2. Encoding Categorical Features:
X_train = train_data.drop(columns=['triglyceride_lvl', 'candidate_id'])
y_train = train_data['triglyceride_lvl']

# Impute missing values in the training data after encoding
imputer = SimpleImputer(strategy='mean')
X_train = imputer.fit_transform(X_train)

# Create column transformer to handle different encoding types
preprocessor = ColumnTransformer(
    transformers=[
        ('onehot', OneHotEncoder(drop='first'), ['gender', 'smoking_habit', 'drinking_habit', 'residential_area']),
        ('ordinal_left', OrdinalEncoder(categories=[['Normal', 'Slightly Defective', 'Highly Defective']]), ['can_hear_left_ear']),
        ('ordinal_right', OrdinalEncoder(categories=[['Normal', 'Slightly Defective', 'Highly Defective']]), ['can_hear_right_ear'])
    ],
    remainder='passthrough'  # Passthrough for numerical columns
)


# Split into features (X) and target (y)
X_train = train_data.drop(columns=['triglyceride_lvl', 'candidate_id'])
y_train = train_data['triglyceride_lvl']

# Apply the column transformer to the data
train_data_encoded = preprocessor.fit_transform(X_train)

X_train_encoded = preprocessor.fit_transform(X_train)

# Convert the encoded data back to a DataFrame for easier manipulation later
X_train = pd.DataFrame(train_data_encoded, columns=preprocessor.get_feature_names_out())

# Create test data matrix (excluding candidate_id and triglyceride_lvl)
X_test = test_data.drop(columns=['candidate_id'])

# Apply the same transformations to the test data (use 'transform', not 'fit_transform')
test_data_encoded = preprocessor.transform(X_test)
X_test = pd.DataFrame(test_data_encoded, columns=preprocessor.get_feature_names_out())
X_test_encoded = preprocessor.transform(X_test.drop(columns=['candidate_id'])) # remove candidate id in test data as well

# Apply the same transformations to the test data (use 'transform', not 'fit_transform')
X_test = imputer.transform(X_test)

# 3. Feature Scaling:
# Create scaler object
scaler = StandardScaler()

# Fit the scaler to the training data and transform
train_data_scaled = scaler.fit_transform(X_train)

# Transform the test data
test_data_scaled = scaler.transform(X_test)

# Convert back to DataFrames
X_train = pd.DataFrame(train_data_scaled, columns=X_train.columns)
X_test = pd.DataFrame(test_data_scaled, columns=X_test.columns)

# Splitting the data into train and validation sets
X_train_final, X_val, y_train_final, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)
X_val = imputer.transform(X_val)

# Model Building
model_lr = LinearRegression()  # Create a Linear Regression model
model_lr.fit(X_train_final, y_train_final)  # Train the model on the training data

# Prediction and Evaluation on the Validation Set
y_pred_val = model_lr.predict(X_val)
mae = mean_absolute_error(y_val, y_pred_val)
score = max(0, 100 - mae)

print("Linear Regression MAE on Validation Set:", mae)
print("Linear Regression Score on Validation Set:", score)

# Prediction on the Test Set (Uncomment when ready to submit)
# y_pred_test = model_lr.predict(X_test)
# Create submission file (Uncomment when ready to submit)
# submission = pd.DataFrame({"candidate_id": test_data["candidate_id"], "triglyceride_lvl": y_pred_test})
# submission.to_csv("submission.csv", index=False)

